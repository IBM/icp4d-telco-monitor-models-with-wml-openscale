{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Monitor your ML Models using Watson OpenScale and WML on Cloud Pak for Data"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Setup the Notebook Environment"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.1 Install the necessary packages"}, {"metadata": {}, "cell_type": "markdown", "source": "### Watson OpenScale Python SDK"}, {"metadata": {}, "cell_type": "code", "source": "!pip install ibm-ai-openscale", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Scikit-learn version 0.20"}, {"metadata": {}, "cell_type": "code", "source": "!pip install scikit-learn==0.20.3", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Watson Machine Learning Python SDK"}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade watson-machine-learning-client-V4==1.0.93 | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Restart the Notebook after Installing the required packages. By clicking on `Kernel>Restart`"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.2 Import Packages"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn import preprocessing\nfrom sklearn import svm, metrics\nfrom scipy import sparse\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, Imputer, OneHotEncoder\nimport json\nimport ibm_db\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom ibm_ai_openscale import APIClient4ICP\nfrom ibm_ai_openscale.engines import *\nfrom ibm_ai_openscale.utils import *\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord, Feature\nfrom ibm_ai_openscale.supporting_classes.enums import *", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Configuration"}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Global Variables"}, {"metadata": {}, "cell_type": "code", "source": "MODEL_NAME=\"Tower A\"\nDEPLOYMENT_NAME=\"Tower A Dep\"\n# Ensure you create a an empty Schema and store the name in this variable\nSCHEMA_NAME=\"Dataset\"\n\n# Enter the Deployment Space you have associated project with \ndep_name=\"Telco_Deployment\"\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Add Dataset\n\nSelect the `Insert Pandas Dataframe` option, after selecting the below cell. Ensure the variable name is `df_data_1`"}, {"metadata": {}, "cell_type": "code", "source": "\ndf_data_1 = pd.read_csv('/project_data/data_asset/call_drop_data_train.csv')\ndf_data_1.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Update your AIOS Credentials"}, {"metadata": {}, "cell_type": "code", "source": "WOS_CREDENTIALS={\n    \"url\" : os.environ['RUNTIME_ENV_APSX_URL'],\n    \"username\":\"admin\",\n    \"password\":\"password\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.4 Input your WML Credentials \n"}, {"metadata": {}, "cell_type": "code", "source": "import sys,os,os.path\n\n\nWML_CREDENTIALS = {\n\"token\": os.environ['USER_ACCESS_TOKEN'],\n\"instance_id\" : \"wml_local\",\n\"url\" : os.environ['RUNTIME_ENV_APSX_URL'],\n\"version\": \"3.0.0\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.5 Add your Db credentials\n\n#### These Db credentials are needed ONLY if you have NOT configured your `OpenScale Datamart`."}, {"metadata": {}, "cell_type": "code", "source": "\nDATABASE_CREDENTIALS = {\n    \"hostname\": \"DB-Server-IP\",\n    \"username\": \"DB-username\",\n    \"password\": \"DB-Pwd\",\n    \"port\": 1000,\n    \"db\": \"DB-name\",\n    \n}\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Create the Call Drop Model using Scikit-Learn"}, {"metadata": {}, "cell_type": "code", "source": "X=df_data_1.drop(['Call_Drop_Actual'], axis=1)\ny=df_data_1.loc[:, 'Call_Drop_Actual']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "'''Add a categorical transformer to your model pipeline. \n    You will need to add a label encoder into the model pipeline before storing it into WML '''\n\ncategorical_features = [\"Start_Time_MM_DD_YYYY\", \"Traffic\", \" _conds\", \"Start_Time_HH_MM_SS_s\"]\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', svm.SVC(kernel='linear'))])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(X_train,y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "meta_props={\n client.repository.ModelMetaNames.NAME: MODEL_NAME,\n client.repository.ModelMetaNames.RUNTIME_UID: \"scikit-learn_0.20-py3.6\",\n client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.20\",\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Create a new Deployment Space"}, {"metadata": {}, "cell_type": "code", "source": "\nproject_id = os.environ['PROJECT_ID']\nclient.set.default_project(project_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def guid_from_space_name(client, space_name):\n\n    instance_details = client.service_instance.get_details()\n\n    space = client.spaces.get_details()\n    res=[]\n    for item in space['resources']: \n        if item['entity'][\"name\"] == space_name:\n            res=item['metadata']['guid']\n\n    return res", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Enter the name of your deployment space of the current project\n\nspace_uid = guid_from_space_name(client, dep_name)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nspace_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nclient.set.default_space(space_uid)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Store, Deploy and Score your Custom WML Model"}, {"metadata": {}, "cell_type": "code", "source": "\ndeploy_meta = {\n     client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n     client.deployments.ConfigurationMetaNames.ONLINE: {}\n }", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Store the model on WML\npublished_model = client.repository.store_model(pipeline,\n                                             meta_props=meta_props,\n                                             training_data=X_train,\n                                             training_target=y_train\n                                                )\n\n\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "published_model_uid = client.repository.get_model_uid(published_model)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Create a Deployment for your stored model\n\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=deploy_meta)\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nscoring_endpoint = None\ndeployment_uid=created_deployment['metadata']['guid']\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Setup your Watson Openscale Dashboard "}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1 Create the Watson Openscale Client"}, {"metadata": {}, "cell_type": "code", "source": "ai_client = APIClient4ICP(aios_credentials=WOS_CREDENTIALS)\nai_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2 Setup the Datamart on AI OpenScale"}, {"metadata": {}, "cell_type": "code", "source": "try:\n    data_mart_details = ai_client.data_mart.get_details()\n    print('Using existing external datamart')\nexcept:\n    print('Setting up external datamart')\n    ai_client.data_mart.setup(db_credentials=DATABASE_CREDENTIALS, schema=SCHEMA_NAME)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_mart_details = ai_client.data_mart.get_details()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_mart_details", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.3 Add your Machine Learning Provider\n\nIf you have already bound the ML Provider to the Openscale instance, then just retrieve the binding_uid, by commenting first line and uncommenting the second line"}, {"metadata": {}, "cell_type": "code", "source": "WML_CREDENTIALS", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "binding_uid = ai_client.data_mart.bindings.add('WML instance - 1', WatsonMachineLearningInstance4ICP(wml_credentials=WML_CREDENTIALS))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ai_client.data_mart.bindings.list_assets()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.4 Perform Initial Scoring for your Model Deployment\n"}, {"metadata": {}, "cell_type": "code", "source": "score=X_test.tail(20)\nscore", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\nscoring_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fields=list(X_test.columns)\nprint(len(fields))\nfields, scoring_data[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\njob_payload = {\nclient.deployments.ScoringMetaNames.INPUT_DATA: [{\n 'values': scoring_data\n}]\n}\nprint(job_payload)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_response = client.deployments.score(deployment_uid, job_payload)\n\nprint(scoring_response)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.5 Create a new Subscription "}, {"metadata": {}, "cell_type": "code", "source": "subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n    published_model_uid,\n    problem_type=ProblemType.BINARY_CLASSIFICATION,\n    input_data_type=InputDataType.STRUCTURED,\n    label_column='Call_Drop_Actual',\n    prediction_column='prediction',\n    probability_column='prediction_probability',\n    categorical_columns=[\"Start_Time_MM_DD_YYYY\",\"Start_Time_HH_MM_SS_s\",\" _conds\",\"Traffic\"],\n    feature_columns = [\"outgoing_site_id\",\"Start_Time_MM_DD_YYYY\",\"Start_Time_HH_MM_SS_s\",\"Call_Service_Duration\",\" _conds\",\" _dewptm\",\" _fog\",\" _hail\",\" _hum\",\" _pressurem\",\"total number_of_calls\",\"total call duration (min)\",\"Traffic\",\"lat\",\"long\",\"Call_Drop_Count\",\"Total_Calls\",\"Call_Drop_Perc\"],\n))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\nai_client.data_mart.subscriptions.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.6 Perform Inital Payload Logging\nNote: You may re-use this code snippet by modifying the request_data variable to perform payload logging after finishing the initial dashboard setup"}, {"metadata": {}, "cell_type": "code", "source": "fields=list(X_test.columns)\n\nrequest_data = {\n    \"fields\": fields,\n    \"values\": scoring_data\n  }\nrequest_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## From the output of the above table choose your model name and copy the uid against it. Store the uid in the subscription_uid variable\n\n\nsubscription_uid=\"90abd408-388f-4543-9700-7139799c620c\"\nfrom ibm_ai_openscale import APIClient4ICP\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord\n\n\nsubscription = ai_client.data_mart.subscriptions.get(subscription_uid=subscription_uid)\n\n\"\"\"\nrequest_data - input to scoring endpoint in supported by Watson OpenScale format\nresponse_data - output from scored model in supported by Watson OpenScale format\nresponse_time - scoring request response time [ms] (integer type)\n\nExample:\n\nrequest_data = {\n    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\"],\n    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026]]\n  }\n\nresponse_data = {\n    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\", \"probability\", \"prediction\", \"DRUG\"],\n    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026, [0.82, 0.07, 0.0, 0.05, 0.03], 0.0, \"drugY\"]]\n  }\n\"\"\"\n\n\n\nrecords = [PayloadRecord(request=request_data, response=scoring_response, response_time=18), \n                PayloadRecord(request=request_data, response=scoring_response, response_time=12)]\n\nsubscription.payload_logging.store(records=records)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.7 Setup Quality Monitoring\n\n```NOTE: If you are using the dataset provided in the dashboard, leave the threshold monitors to these values. However, if you are using your own dataset, you can play around with the threshold value (value b/w 0 and 1) according to your requirement.```"}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(5)\nsubscription.quality_monitoring.enable(threshold=0.95, min_records=5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.8 Log Feedback Data to your Subscription"}, {"metadata": {}, "cell_type": "code", "source": "feedback_data_raw=pd.concat([X_test,y_test],axis=1)\nfeedback_data_raw", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "feedback_data=feedback_data_raw.tail(20).values.tolist()\nfeedback_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "feedback_scoring={\n    \"data\":feedback_data\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nsubscription.feedback_logging.store(feedback_scoring['data'])\n\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.feedback_logging.show_table()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Run an inital quality test"}, {"metadata": {}, "cell_type": "code", "source": "run_details = subscription.quality_monitoring.run(background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.quality_monitoring.show_table()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\n\nquality_pd = subscription.quality_monitoring.get_table_content(format='pandas')\nquality_pd.plot.barh(x='id', y='value');", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.9 Setup the Fairness Monitors\n\nThe code below configures fairness monitoring for our model. It turns on monitoring for two features, _conds(Weather Condition) and Traffic for the cell tower. In each case, we must specify:\n  * Which model feature to monitor\n  * One or more **majority** groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n  * One or more **minority** groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n  * The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 95%)\n\nAdditionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 5 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."}, {"metadata": {}, "cell_type": "code", "source": "subscription.fairness_monitoring.enable(\n            features=[\n                Feature(\"Traffic\", majority=['Low'], minority=['High','Medium'], threshold=0.95),\n                Feature(\" _conds\", majority=['Haze','Rain'], minority=['Clear','Fog','Partly Cloudy'], threshold=0.95)\n            ],\n            favourable_classes=[1],\n            unfavourable_classes=[0],\n            min_records=5,\n            training_data=df_data_1\n        )", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(5)\n\nrun_details = subscription.fairness_monitoring.run(background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(5)\n\nsubscription.fairness_monitoring.show_table()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Add some more Payload (Optional for populating your dashboard)\n\nIf you wish to add some Payload Data. Take different sections of your test dataset and send to OpenScale as shown below-"}, {"metadata": {}, "cell_type": "code", "source": "score=X_test.head(100)\nscore", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\nscoring_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fields=list(X_test.columns)\nprint(len(fields))\nfields, scoring_data[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "request_data = {\n    \"fields\": fields,\n    \"values\": scoring_data\n  }\nrequest_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## From the output of the above table choose your model name and copy the uid against it. Store the uid in the subscription_uid variable\n\n\n\nfrom ibm_ai_openscale import APIClient4ICP\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord\n\n\nsubscription = ai_client.data_mart.subscriptions.get(subscription_uid=subscription_uid)\n\n\"\"\"\nrequest_data - input to scoring endpoint in supported by Watson OpenScale format\nresponse_data - output from scored model in supported by Watson OpenScale format\nresponse_time - scoring request response time [ms] (integer type)\n\nExample:\n\nrequest_data = {\n    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\"],\n    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026]]\n  }\n\nresponse_data = {\n    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\", \"probability\", \"prediction\", \"DRUG\"],\n    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026, [0.82, 0.07, 0.0, 0.05, 0.03], 0.0, \"drugY\"]]\n  }\n\"\"\"\n\n\n\nrecords = [PayloadRecord(request=request_data, response=scoring_response, response_time=18), \n                PayloadRecord(request=request_data, response=scoring_response, response_time=12)]\n\nsubscription.payload_logging.store(records=records)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}